{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HKuwKaoLWXQn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "# some utility functions to create toy data\n",
    "# data ~ underlying function + gaussian noise\n",
    "\n",
    "def create_toy_data(func, sample_size, std):\n",
    "    x = np.linspace(0, 1, sample_size).reshape(-1, 1)\n",
    "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
    "    return x, t\n",
    "\n",
    "def a_sinusoidal_func(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "def a_polynomial_func(x):\n",
    "    return (12. - 6.14*x + 8.4*x*x)\n",
    "\n",
    "def an_exp_func(x):\n",
    "    return (1+1*np.exp(0.001*x))\n",
    "\n",
    "def a_linear_func(x):\n",
    "    return (1.17 + 3.14*x)\n",
    "\n",
    "def a_discontinuous_func(x):\n",
    "    return [ 1. if el>0.5 else 0. for el in x  ]\n",
    "    \n",
    "sample_size = 10\n",
    "sigma = 0.3\n",
    "\n",
    "func = a_sinusoidal_func\n",
    "#func = a_polynomial_func\n",
    "#func = a_discontinuous_func\n",
    "#func = a_linear_func\n",
    "\n",
    "np.random.seed(1234)\n",
    "x_train, y_train = create_toy_data(func, sample_size, sigma)\n",
    "x_predict = np.linspace(0, 1, 100).reshape(-1, 1) #  some x-vals so we can generate true y-vals\n",
    "y_true = func(x_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I-i-KWaEXSOS",
    "outputId": "9fcc3464-7fcf-4347-fd88-871f4771bf2d"
   },
   "outputs": [],
   "source": [
    "# this is what the x variable looks like...\n",
    "# just a progression of numbers between 0 and 1\n",
    "# that forms the basis of the sinusoid data\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPBVJzabWXQp",
    "outputId": "1763f9d3-6e79-4b99-f404-20daf813a135"
   },
   "outputs": [],
   "source": [
    "# lets take a look at how we can transform x_train \n",
    "# into a polynomial feature set\n",
    "feature = PolynomialFeatures(degree=3)\n",
    "X_train = feature.fit_transform(x_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540
    },
    "id": "7oB6CWeRWXQp",
    "outputId": "6906ad0b-d56c-401c-faf3-84a4d7a55738"
   },
   "outputs": [],
   "source": [
    "# make a graph with 2x2 subplots\n",
    "fig, axes = plt.subplots(2,2,sharex=True, sharey=True,figsize=(10, 8))\n",
    "axs = axes.flatten()\n",
    "\n",
    "# loop through fitting/plotting 0th, 1st, 3rd, and 9th order polynomials\n",
    "for i, degree in enumerate([ 0, 1, 3, 9]):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    feature = ## insert your code here to make polynomail features transformer ##\n",
    "    X_train = ## turn x_train into polynomial features ## \n",
    "    X_predict = ## turn x_predict into polynomial features ##\n",
    "\n",
    "    model = ## a linear regression with fit_intercept set to False ##\n",
    "    ## hey! why do we do that False argument above????  think about it! ##\n",
    "    \n",
    "    ## insert your code to fit the model here ##\n",
    "    y_predict = ## predict the outputs from X_predict ##\n",
    "\n",
    "    ax.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n",
    "    ax.plot(x_predict, y_true, c=\"g\", label=\"true function\")\n",
    "    ax.plot(x_predict, y_predict, c=\"r\", label=\"fitted curve\")\n",
    "    ax.annotate(\"M={}\".format(degree), xy=(.15, .05),  xycoords='axes fraction', fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)\n",
    "plt.suptitle('Polynomial regression with 10 training data points',fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "7HdjPIEmWXQq",
    "outputId": "2bce6271-1b64-461d-b945-71dcd9ea3f1c"
   },
   "outputs": [],
   "source": [
    "# OK let's draw 50 new random data points from the same underlying function as a validation set\n",
    "x_validation, y_validation = create_toy_data(func, 50, sigma)\n",
    "y_validation_true = func(x_validation)\n",
    "\n",
    "fig, axes = plt.subplots(2,2,sharex=True, sharey=True,figsize=(10, 8))\n",
    "axs = axes.flatten()\n",
    "for i, degree in enumerate([0, 1, 3, 9]):\n",
    "    ax = axs[i]\n",
    " \n",
    "    ## copy paste the entire code block you made above, down below here\n",
    "    feature = ## insert your code here to make polynomail features transformer ##\n",
    "    X_train = ## turn x_train into polynomial features ## \n",
    "    X_predict = ## turn x_predict into polynomial features ##\n",
    "\n",
    "    model = ## a linear regression with fit_intercept set to False ##\n",
    "    ## hey! why do we do that False argument above????  think about it! ##\n",
    "    \n",
    "    ## insert your code to fit the model here ##\n",
    "    y_predict = ## predict the outputs from X_predict ##\n",
    "\n",
    "    ax.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n",
    "    ax.scatter(x_validation, y_validation, facecolor=\"none\", edgecolor=\"y\", s=50, label=\"validation data\")\n",
    "    ax.plot(x_predict, y, c=\"r\", label=\"fitted curve\")\n",
    "    ax.annotate(\"M={}\".format(degree), xy=(.15, .05),  xycoords='axes fraction', fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "u2JGjLb_WXQq",
    "outputId": "f9f980a1-4496-4f95-882a-0518992032f9"
   },
   "outputs": [],
   "source": [
    "def rmse(a, b):\n",
    "    return np.sqrt(np.mean(np.square(a - b)))\n",
    "\n",
    "def mse(a,b):\n",
    "    return np.mean(np.square(a - b))\n",
    "\n",
    "def rss(a,b):\n",
    "    return ((a-b).T.dot(a-b))[0]\n",
    "\n",
    "# start with RSS, then see MSE/RMSE have same shape but are normalized against n of dataset\n",
    "lossf = rss\n",
    "\n",
    "training_errors = []\n",
    "validation_errors = []\n",
    "\n",
    "for i in range(10):\n",
    "    feature = PolynomialFeatures(i)\n",
    "    X_train = feature.fit_transform(x_train)\n",
    "    X_validation = feature.fit_transform(x_validation) # 50 validation points drawn from same function\n",
    "\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    training_errors.append(lossf(model.predict(X_train), y_train))\n",
    "    validation_errors.append(lossf(model.predict(X_validation), y_validation ))\n",
    "\n",
    "\n",
    "plt.plot(training_errors, 'o-', mfc=\"none\", mec=\"b\", ms=10, c=\"b\", label=\"Training\")\n",
    "plt.plot(validation_errors, 'o-', mfc=\"none\", mec=\"r\", ms=10, c=\"r\", label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"degree\")\n",
    "plt.ylabel(lossf.__name__.upper())\n",
    "plt.title('with 10 training samples and 50 validation samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBnmpOyWWXQr"
   },
   "source": [
    "One technique that is often used to control the over-fitting phenomenon in such cases is that of **regularization**, which involves adding a penalty term to the error function below in order to discourage the coefficients from reaching large values. By preventing the sum of our weights from growing large, we are preventing complex fitting... the total amount of weight allowed will be preferentially allocated to the most important features, preventing features that have less effect on the answer from getting too much love from the algorithm.\n",
    "\n",
    "The simplest such penalty term takes the form of a sum of squares of all of the coefficients, leading to a modified loss/error function of the form\n",
    "\n",
    "$ L(\\mathbf{w}) = (X\\mathbf{w}-\\mathbf{y})^T (X\\mathbf{w}-\\mathbf{y}) + {\\lambda \\over 2} \\lVert \\mathbf{w} \\rVert_2 $\n",
    "\n",
    "where the coefficient $\\lambda$ governs the relative importance of the regularization term compared with the sum-of-squares error term and $\\mathbf{X}$ is the (nxm) design matrix and $\\mathbf{w}$ is an m long column vector and $\\mathbf{y}$ is an n long column vector. \n",
    "\n",
    "There is a closed-form solution below:\n",
    "\n",
    "$\\mathbf{w}^* = (X^T X + \\lambda I)^{-1} X^T y$\n",
    "\n",
    "So you can plug that into numpy and get the same answer as the sklearn quickie below.   If you want to derive the closed form solution yourself, here's a hint:$ \\lVert W \\rVert_2 = W^TW$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XozbqF8xWXQr"
   },
   "outputs": [],
   "source": [
    " # L2 regularized least squares regression is called ridge regression or sometimes Tikhonov regularization\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "fig, axes = plt.subplots(2,2,sharex=True, sharey=True,figsize=(10, 8))\n",
    "axs = axes.flatten()\n",
    "for i, lmda in enumerate([ 1e-1, 1e-3, 1e-6, 1e-9]):\n",
    "    ax = axs[i]\n",
    "    feature = PolynomialFeatures(degree)\n",
    "    X_train = feature.fit_transform(x_train)\n",
    "    X_predict = feature.fit_transform(x_predict)\n",
    "\n",
    "    model = Ridge(lmda,fit_intercept=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_predict)\n",
    "\n",
    "    ax.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n",
    "    ax.plot(x_predict, y_true, c=\"g\", label=\"true function\")\n",
    "    ax.plot(x_predict, y_predict, c=\"r\", label=\"fitted curve\")\n",
    "    ax.annotate(\"$m=9,\\lambda$={:3.1e}\".format(lmda), xy=(.15, .05),  xycoords='axes fraction', fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)\n",
    "plt.suptitle('9th order polynomial regression + L2 regularization',fontsize=20)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySwMudjaWXQr"
   },
   "source": [
    "Now a few more words about various kinds of regularizations.  Ridge or L2 regularization is what we discussed above it has the form:\n",
    "$$    L(\\mathbf{w}) = (X\\mathbf{w}-\\mathbf{y})^T (X\\mathbf{w}-\\mathbf{y}) + {\\lambda \\over 2} \\lVert \\mathbf{w} \\rVert_2 $$\n",
    "\n",
    "You can also do L1 regularization which is often called LASSO regression (least absolute shrinkage and selection operator)\n",
    "$$    L(\\mathbf{w}) = (X\\mathbf{w}-\\mathbf{y})^T (X\\mathbf{w}-\\mathbf{y}) + {\\lambda \\over 2} \\lVert \\mathbf{w} \\rVert_1 $$\n",
    "\n",
    "And you can combine the two together in a technique called ElasticNet\n",
    "$$    L(\\mathbf{w}) = (X\\mathbf{w}-\\mathbf{y})^T (X\\mathbf{w}-\\mathbf{y}) + {\\lambda \\over 2} ( \\alpha \\lVert \\mathbf{w} \\rVert_1 + (1-\\alpha) \\lVert \\mathbf{w} \\rVert_2 ) $$\n",
    "\n",
    "where $\\alpha  \\in [0,1]$ is a parameter dictating the proportion of L1 to L2 regularization.\n",
    "\n",
    "Why all these different kinds of regularization? Well L1 tends to produce a *sparse* solution... many weights that are not important are driven towards 0.  You use this technique when it seems appropriate to you that less-important factors have no influence on the solution.  Whereas L2 limits the total amount of weight evenly, so less-important factors can continue to have less-influence-but-still-some-influence.\n",
    "\n",
    "One application of L1 regularization: feature selection. Let's say you have data about the expression levels of ~27,000 protein coding genes across a few thousand humans, and you want to dermine if any of the genes have an effect on a disease.  Since almost NONE of them will, it's good to use something like a heavy L1 penalty, which will prevent you from picking up too much on random chance associations that may exist in the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAZTN5zLWXQs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
